{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONCPlw+2P3tKM8mOn/fMB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavoNicodemos/Algoritmo_analise_resultado/blob/main/Algoritmo_Analise_GUV_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hbCQpxS8XuzQ"
      },
      "outputs": [],
      "source": [
        "!pip install pyxlsb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "\n",
        "# ==============================================================\n",
        "# Classe principal para análise contábil automatizada\n",
        "# ==============================================================\n",
        "class AccountingAnalyzer:\n",
        "    \"\"\"\n",
        "    Classe para análise contábil automatizada:\n",
        "    - Detecta anomalias, agrupa clusters e gera explicações.\n",
        "    - Consolida dados por \"Group_Account\" e calcula variações.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Modelos utilizados na análise\n",
        "        self.anomaly_detector = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
        "        self.clusterer = MiniBatchKMeans(n_clusters=5, random_state=42)\n",
        "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        self.classifier = xgb.XGBClassifier()\n",
        "        self.history = pd.DataFrame()  # Armazena dados já analisados\n",
        "        self.treinou = False  # Indica se o modelo foi treinado\n",
        "\n",
        "    def fit(self, historical_data):\n",
        "        \"\"\"\n",
        "        Treina o modelo de detecção de anomalias com base em dados históricos.\n",
        "        \"\"\"\n",
        "        features = self._preprocess(historical_data)\n",
        "        self.anomaly_detector.fit(features)\n",
        "        self.treinou = True\n",
        "        print(\"✅ Modelo treinado com sucesso com base histórica.\")\n",
        "\n",
        "    def analyze(self, new_data):\n",
        "        \"\"\"Analisa os novos dados com o modelo previamente treinado.\"\"\"\n",
        "        if not self.treinou:\n",
        "            raise ValueError(\"❌ O modelo precisa ser treinado antes da análise. Use o método .fit() com dados históricos.\")\n",
        "\n",
        "        # Remove duplicados com base no histórico já analisado\n",
        "        new_data = self._check_history(new_data)\n",
        "\n",
        "        # Pré-processamento (mesmas transformações do treino)\n",
        "        features = self._preprocess(new_data)\n",
        "\n",
        "        # Detectar anomalias\n",
        "        new_data['anomaly'] = self.anomaly_detector.fit_predict(features)\n",
        "\n",
        "        # Agrupar em clusters\n",
        "        new_data['cluster'] = self.clusterer.fit_predict(features)\n",
        "\n",
        "        # Calcular variações entre períodos\n",
        "        new_data = self._calculate_variations(new_data)\n",
        "\n",
        "        # Gerar explicações automáticas\n",
        "        new_data['explanation'] = self._generate_explanations(new_data)\n",
        "\n",
        "        # Consolidar por \"Group_Account\" e ordenar como DRE\n",
        "        consolidated_data = self._consolidate_data(new_data)\n",
        "\n",
        "        # Atualiza o histórico\n",
        "        self._update_history(new_data)\n",
        "\n",
        "        # Retorna relatório consolidado\n",
        "        return {\n",
        "            'anomalias_df': new_data[new_data['anomaly'] == -1],\n",
        "            'consolidated_df': consolidated_data,\n",
        "            'explicacoes_df': new_data[['Group_Account', 'explanation']]\n",
        "        }\n",
        "\n",
        "    def _check_history(self, data):\n",
        "        \"\"\"Remove dados já analisados anteriormente.\"\"\"\n",
        "        if self.history.empty:\n",
        "            return data\n",
        "        return data[~data.index.isin(self.history.index)]\n",
        "\n",
        "    def _preprocess(self, data):\n",
        "        \"\"\"Codifica dados categóricos e prepara os dados numéricos.\"\"\"\n",
        "        cat_data = self.encoder.fit_transform(data[['Cost_Center', 'Group_Account']])\n",
        "        num_data = data[['Amount_in_LC']].values\n",
        "        return np.hstack([num_data, cat_data])\n",
        "\n",
        "    def _calculate_variations(self, data):\n",
        "        \"\"\"Calcula variações absolutas e percentuais entre períodos.\"\"\"\n",
        "        data['previous_amount'] = data.groupby('Group_Account')['Amount_in_LC'].shift(1)\n",
        "        data['absolute_variation'] = data['Amount_in_LC'] - data['previous_amount']\n",
        "        data['percent_variation'] = (data['absolute_variation'] / data['previous_amount']) * 100\n",
        "        return data\n",
        "\n",
        "    def _generate_explanations(self, data):\n",
        "        \"\"\"Gera justificativas automáticas para anomalias e variações.\"\"\"\n",
        "        explanations = []\n",
        "        for _, row in data.iterrows():\n",
        "            if row['anomaly'] == -1:\n",
        "                explanations.append(f\"Anomalia detectada em {row['Group_Account']}.\")\n",
        "            elif pd.notnull(row['absolute_variation']) and abs(row['percent_variation']) > 10:\n",
        "                explanations.append(f\"Variação significativa de {row['percent_variation']:.2f}% em {row['Group_Account']}.\")\n",
        "            else:\n",
        "                explanations.append(\"Sem anomalias ou variações significativas.\")\n",
        "        return explanations\n",
        "\n",
        "    def _consolidate_data(self, data):\n",
        "        \"\"\"Consolida os dados por 'Group_Account' e ordena como DRE.\"\"\"\n",
        "        consolidated = data.groupby('Group_Account').agg({\n",
        "            'Amount_in_LC': 'sum',\n",
        "            'absolute_variation': 'sum',\n",
        "            'percent_variation': 'mean'\n",
        "        }).reset_index()\n",
        "        return consolidated.sort_values(by='Group_Account')\n",
        "\n",
        "    def _update_history(self, data):\n",
        "        \"\"\"Salva os dados já analisados para evitar repetições futuras.\"\"\"\n",
        "        self.history = pd.concat([self.history, data], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Execução do algoritmo\n",
        "# ==============================================================\n",
        "!pip install pyxlsb\n",
        "if __name__ == \"__main__\":\n",
        "    # Carregar dados\n",
        "    base_dados = pd.read_excel('Dados_GUV.xlsb')\n",
        "    dados_historicos = pd.read_excel('Dados_historicos_GUV.xlsb')\n",
        "\n",
        "    # Instanciar e treinar o analisador\n",
        "    analisador = AccountingAnalyzer()\n",
        "    analisador.fit(dados_historicos)\n",
        "\n",
        "    # Analisar novos dados\n",
        "    relatorio = analisador.analyze(base_dados)\n",
        "\n",
        "    # Salvar outputs em Excel\n",
        "    relatorio['anomalias_df'].to_excel('anomalias_df.xlsx', index=False)\n",
        "    relatorio['consolidated_df'].to_excel('consolidated_df.xlsx', index=False)\n",
        "    relatorio['explicacoes_df'].to_excel('explicacoes_df.xlsx', index=False)\n",
        "\n",
        "    print(\"✅ Relatórios gerados com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuEb1Zl5X30y",
        "outputId": "380e7a92-0fab-4f8b-e616-8c50fdadf925"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyxlsb\n",
            "  Downloading pyxlsb-1.0.10-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading pyxlsb-1.0.10-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyxlsb\n",
            "Successfully installed pyxlsb-1.0.10\n",
            "✅ Modelo treinado com sucesso com base histórica.\n",
            "✅ Relatórios gerados com sucesso!\n"
          ]
        }
      ]
    }
  ]
}