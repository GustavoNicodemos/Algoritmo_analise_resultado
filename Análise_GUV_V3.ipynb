{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzCI7FaXUd68eu54l9Uwps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavoNicodemos/Algoritmo_analise_resultado/blob/main/An%C3%A1lise_GUV_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V8miSxrDkXk"
      },
      "outputs": [],
      "source": [
        "# Instale o pacote necessário\n",
        "!pip install pyxlsb openpyxl\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "\n",
        "class AccountingAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.anomaly_detector = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
        "        self.clusterer = MiniBatchKMeans(n_clusters=5, random_state=42)\n",
        "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        self.classifier = xgb.XGBClassifier()\n",
        "        self.history = pd.DataFrame()\n",
        "        self.treinou = False\n",
        "\n",
        "    def fit(self, historical_data):\n",
        "        features = self._preprocess(historical_data)\n",
        "        self.anomaly_detector.fit(features)\n",
        "        self.historical_data = historical_data.copy()\n",
        "        self.treinou = True\n",
        "        print(\"✅ Modelo treinado com sucesso com base histórica.\")\n",
        "\n",
        "    def analyze(self, new_data):\n",
        "        if not self.treinou:\n",
        "            raise ValueError(\"❌ O modelo precisa ser treinado antes da análise. Use o método .fit() com dados históricos.\")\n",
        "\n",
        "        new_data = self._check_history(new_data)\n",
        "        features = self._preprocess(new_data)\n",
        "\n",
        "        new_data['anomaly'] = self.anomaly_detector.predict(features)\n",
        "        new_data['cluster'] = self.clusterer.fit_predict(features)\n",
        "\n",
        "        new_data = self._calculate_variations(new_data)\n",
        "\n",
        "        new_data['explanation'] = self._generate_explanations(new_data)\n",
        "\n",
        "        consolidated_data = self._consolidate_data(new_data)\n",
        "\n",
        "        self._update_history(new_data)\n",
        "\n",
        "        # Nova funcionalidade: análise DRE com desvio\n",
        "        variacoes_relevantes, detalhes_variacoes = self._detect_significant_variations(new_data)\n",
        "\n",
        "        return {\n",
        "            'anomalias_df': new_data[new_data['anomaly'] == -1],\n",
        "            'consolidated_df': consolidated_data,\n",
        "            'explicacoes_df': new_data[['Group_Account', 'explanation']],\n",
        "            'variacoes_relevantes': variacoes_relevantes,\n",
        "            'detalhes_variacoes': detalhes_variacoes\n",
        "        }\n",
        "\n",
        "    def _check_history(self, data):\n",
        "        if self.history.empty:\n",
        "            return data\n",
        "        return data[~data.index.isin(self.history.index)]\n",
        "\n",
        "    def _preprocess(self, data):\n",
        "        cat_data = self.encoder.fit_transform(data[['Cost_Center', 'Group_Account']])\n",
        "        num_data = data[['Amount_in_LC']].values\n",
        "        return np.hstack([num_data, cat_data])\n",
        "\n",
        "    def _calculate_variations(self, data):\n",
        "        data = data.sort_values(by=['Cost_Center', 'Group_Account', 'Period'])\n",
        "        data['previous_amount'] = data.groupby(['Cost_Center', 'Group_Account'])['Amount_in_LC'].shift(1)\n",
        "        data['absolute_variation'] = data['Amount_in_LC'] - data['previous_amount']\n",
        "        data['percent_variation'] = (data['absolute_variation'] / data['previous_amount'].replace(0, np.nan)) * 100\n",
        "        return data\n",
        "\n",
        "    def _generate_explanations(self, data):\n",
        "        explanations = []\n",
        "        for _, row in data.iterrows():\n",
        "            if row['anomaly'] == -1:\n",
        "                explanations.append(f\"Anomalia detectada em {row['Group_Account']}.\")\n",
        "            elif pd.notnull(row['absolute_variation']) and abs(row['percent_variation']) > 10:\n",
        "                explanations.append(f\"Variação significativa de {row['percent_variation']:.2f}% em {row['Group_Account']}.\")\n",
        "            else:\n",
        "                explanations.append(\"Sem anomalias ou variações significativas.\")\n",
        "        return explanations\n",
        "\n",
        "    def _consolidate_data(self, data):\n",
        "        consolidated = data.groupby('Group_Account').agg({\n",
        "            'Amount_in_LC': 'sum',\n",
        "            'absolute_variation': 'sum',\n",
        "            'percent_variation': 'mean'\n",
        "        }).reset_index()\n",
        "        return consolidated.sort_values(by='Group_Account')\n",
        "\n",
        "    def _update_history(self, data):\n",
        "        self.history = pd.concat([self.history, data], ignore_index=True)\n",
        "\n",
        "    def _detect_significant_variations(self, new_data):\n",
        "        df_historico = self.historical_data.copy()\n",
        "        df_novo = new_data.copy()\n",
        "\n",
        "        df_historico['Period'] = pd.to_datetime(df_historico['Period'])\n",
        "        df_novo['Period'] = pd.to_datetime(df_novo['Period'])\n",
        "\n",
        "        # DRE histórica por Cost Center e Group Account\n",
        "        dre_hist = df_historico.groupby(['Cost_Center', 'Group_Account', 'Period'])['Amount_in_LC'].sum().reset_index()\n",
        "        desvio = dre_hist.groupby(['Cost_Center', 'Group_Account'])['Amount_in_LC'].std().reset_index()\n",
        "        desvio.columns = ['Cost_Center', 'Group_Account', 'std_dev']\n",
        "\n",
        "        # DRE atual\n",
        "        dre_atual = df_novo.groupby(['Cost_Center', 'Group_Account'])['Amount_in_LC'].sum().reset_index()\n",
        "\n",
        "        comparativo = pd.merge(dre_atual, desvio, on=['Cost_Center', 'Group_Account'], how='left')\n",
        "\n",
        "        comparativo['std_dev'] = comparativo['std_dev'].fillna(0)\n",
        "        comparativo['relevant'] = abs(comparativo['Amount_in_LC']) > (comparativo['std_dev'] * 1.5)\n",
        "\n",
        "        relevantes = comparativo[comparativo['relevant']].copy()\n",
        "\n",
        "        # Detalhamento dos lançamentos responsáveis\n",
        "        detalhes = pd.merge(df_novo, relevantes[['Cost_Center', 'Group_Account']], on=['Cost_Center', 'Group_Account'], how='inner')\n",
        "\n",
        "        return relevantes, detalhes\n",
        "\n",
        "# Execução no Colab\n",
        "if __name__ == \"__main__\":\n",
        "    base_dados = pd.read_excel('Dados_GUV.xlsb', engine='pyxlsb')\n",
        "    dados_historicos = pd.read_excel('Dados_historicos_GUV.xlsb', engine='pyxlsb')\n",
        "\n",
        "    analisador = AccountingAnalyzer()\n",
        "    analisador.fit(dados_historicos)\n",
        "\n",
        "    relatorio = analisador.analyze(base_dados)\n",
        "\n",
        "    with pd.ExcelWriter('Relatorio_Contabil_Analitico.xlsx', engine='openpyxl') as writer:\n",
        "        relatorio['anomalias_df'].to_excel(writer, sheet_name='Anomalias', index=False)\n",
        "        relatorio['consolidated_df'].to_excel(writer, sheet_name='Consolidado_DRE', index=False)\n",
        "        relatorio['explicacoes_df'].drop_duplicates().to_excel(writer, sheet_name='Explicacoes', index=False)\n",
        "        relatorio['variacoes_relevantes'].to_excel(writer, sheet_name='Variações Relevantes', index=False)\n",
        "        relatorio['detalhes_variacoes'].to_excel(writer, sheet_name='Detalhamento Lançamentos', index=False)\n",
        "\n",
        "    print(\"✅ Planilha final gerada com múltiplas abas.\")"
      ]
    }
  ]
}